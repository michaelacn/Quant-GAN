{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCN MODEL implemented in Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Wiese et al., Quant GANs: Deep Generation of Financial Time Series, 2019](https://arxiv.org/abs/1907.06673)\n",
    "\n",
    "For both the generator and the discriminator we used TCNs with skip connections. Inside the TCN architecture temporal blocks were used as block modules. A temporal block consists of two dilated causal convolutions and two PReLUs (He et al., 2015) as activation functions. The primary benefit of using temporal blocks is to make the TCN more expressive by increasing the number of non-linear operations in each block module. A complete definition is given below.\n",
    "\n",
    "**Definition B.1 (Temporal block)**. Let $N_I, N_H, N_O ∈ \\Bbb{N}$ denote the input, hidden and output dimension and let $D,K ∈ \\mathbb{N}$ denote the dilation and the kernel size. Furthermore, let $w_1, w_2$ be two dilated causal convolutional layers with arguments $(N_I, N_H, K, D)$  and $(N_H,N_O,K,D)$ respectively and\n",
    "let $φ_1, φ_2 : \\mathbb{R} → \\mathbb{R}$ be two PReLUs. The function $f : \\mathbb{R}^{N_I×(2D(K−1)+1)} → \\mathbb{R}^{N_O}$ defined by\n",
    "$$f(X) = φ_2 ◦ w_2 ◦ φ_1 ◦ w_1(X)$$\n",
    "is called temporal block with arguments $(N_I,N_H,N_O,K,D)$.\n",
    "\n",
    "The TCN architecture used for the generator and the discriminator in the pure TCN and C-SVNN model is illustrated in Table 3. Table 4 shows the input, hidden and output dimensions of the different models. Here, G abbreviates the generator and D the discriminator. Note that for all models, except the generator of the C-SVNN, the hidden dimension was set to eighty. The kernel size of each temporal block, except the first one, was two. Each TCN modeled a RFS of 127."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg .tg-ik58{background-color:#2f2f2f;border-color:inherit;text-align:left;vertical-align:top}\n",
    ".tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<h3>Table 3</h3>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-ik58\">Module Name</th>\n",
    "    <th class=\"tg-ik58\">Arguments</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Temporal block 1</td>\n",
    "    <td class=\"tg-0pky\">(N<sub>I</sub>, N<sub>H</sub>, N<sub>H</sub>, 1, 1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Temporal block 2</td>\n",
    "    <td class=\"tg-0pky\">(N<sub>I</sub>, N<sub>H</sub>, N<sub>H</sub>, 2, 1)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Temporal block 3</td>\n",
    "    <td class=\"tg-0pky\">(N<sub>I</sub>, N<sub>H</sub>, N<sub>H</sub>, 2, 2)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Temporal block 4</td>\n",
    "    <td class=\"tg-0pky\">(N<sub>I</sub>, N<sub>H</sub>, N<sub>H</sub>, 2, 4)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Temporal block 5</td>\n",
    "    <td class=\"tg-0pky\">(N<sub>I</sub>, N<sub>H</sub>, N<sub>H</sub>, 2, 8)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Temporal block 6</td>\n",
    "    <td class=\"tg-0pky\">(N<sub>I</sub>, N<sub>H</sub>, N<sub>H</sub>, 2, 16)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">Temporal block 7</td>\n",
    "    <td class=\"tg-0pky\">(N<sub>I</sub>, N<sub>H</sub>, N<sub>H</sub>, 2, 32)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0pky\">1 x 1 Convolution</td>\n",
    "    <td class=\"tg-0pky\">(N<sub>H</sub>, N<sub>O</sub>, 1, 1)</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style type=\"text/css\">\n",
    ".tg  {border-collapse:collapse;border-spacing:0;}\n",
    ".tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  overflow:hidden;padding:10px 5px;word-break:normal;}\n",
    ".tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n",
    "  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;background-color:#2f2f2f;}\n",
    ".tg .tg-0lax{text-align:left;vertical-align:top}\n",
    "</style>\n",
    "<h3>Table 4</h3>\n",
    "<table class=\"tg\">\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th class=\"tg-0lax\">Models</th>\n",
    "    <th class=\"tg-0lax\">PureTCN-G</th>\n",
    "    <th class=\"tg-0lax\">Pure TCN-D<br></th>\n",
    "    <th class=\"tg-0lax\">C-SVNN-G</th>\n",
    "    <th class=\"tg-0lax\">C-SVNN_D</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">N<sub>I</sub></td>\n",
    "    <td class=\"tg-0lax\">3</td>\n",
    "    <td class=\"tg-0lax\">1</td>\n",
    "    <td class=\"tg-0lax\">3</td>\n",
    "    <td class=\"tg-0lax\">1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">N<sub>H</sub></td>\n",
    "    <td class=\"tg-0lax\">80</td>\n",
    "    <td class=\"tg-0lax\">80</td>\n",
    "    <td class=\"tg-0lax\">50<br></td>\n",
    "    <td class=\"tg-0lax\">80</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"tg-0lax\">N<sub>O</sub></td>\n",
    "    <td class=\"tg-0lax\">1</td>\n",
    "    <td class=\"tg-0lax\">1</td>\n",
    "    <td class=\"tg-0lax\">2</td>\n",
    "    <td class=\"tg-0lax\">1</td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    \"\"\"A Temporal Block module for Temporal Convolutional Networks (TCNs).\n",
    "\n",
    "    This block consists of two 1D convolutional layers with optional downsampling\n",
    "    and residual connections. It is designed to capture temporal dependencies in\n",
    "    sequential data.\n",
    "\n",
    "    Args:\n",
    "        n_inputs (int): Number of input channels (features) to the block.\n",
    "        n_hidden (int): Number of hidden units (channels) in the intermediate layer.\n",
    "        n_outputs (int): Number of output channels (features) from the block.\n",
    "        kernel_size (int): Size of the convolutional kernel along the temporal axis.\n",
    "        dilation (int): Dilation factor for the convolutional layers. Controls the\n",
    "                       spacing between kernel elements to capture long-range dependencies.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Output tensor of shape (batch_size, n_outputs, sequence_length).\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs, kernel_size, dilation):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "\n",
    "        # First convolutional layer\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=n_inputs,\n",
    "            out_channels=n_hidden,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            dilation=dilation,\n",
    "            padding='same'  # Ensures the output has the same length as the input\n",
    "        )\n",
    "\n",
    "        # Activation function after the first convolution\n",
    "        self.relu1 = nn.PReLU() \n",
    "\n",
    "        # Second convolutional layer\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=n_hidden,\n",
    "            out_channels=n_outputs,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            dilation=dilation,\n",
    "            padding='same' \n",
    "        )\n",
    "\n",
    "        # Activation function after the second convolution\n",
    "        self.relu2 = nn.PReLU()\n",
    "\n",
    "        # Main network: Sequence of layers\n",
    "        self.net = nn.Sequential(self.conv1, self.relu1, self.conv2, self.relu2)\n",
    "\n",
    "        # Downsample layer (used if input and output channels differ)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "\n",
    "        # Initialize weights\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights for convolutional layers.\n",
    "\n",
    "        Weights are initialized using a normal distribution with mean 0 and standard\n",
    "        deviation 0.01. This helps stabilize training and avoid vanishing/exploding gradients.\n",
    "        \"\"\"\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        # Apply downsampling if necessary (to match input and output dimensions)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "\n",
    "        return out + res\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    \"\"\"\n",
    "    A Temporal Convolutional Network (TCN) for sequence modeling.\n",
    "\n",
    "    This implementation stacks multiple causal convolutional blocks \n",
    "    (TemporalBlock) in a sequential manner. Each TemporalBlock can include \n",
    "    convolutions with increasing dilation factors to capture a broader \n",
    "    context in the time dimension.\n",
    "\n",
    "    Args\n",
    "    ----------\n",
    "    input_size : int\n",
    "        Number of channels (features) in the input sequence.\n",
    "    output_size : int\n",
    "        Number of output channels (features) for the final layer.\n",
    "    n_hidden : int, optional\n",
    "        Number of hidden channels used in each TemporalBlock. Default is 80.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size, n_hidden=80):\n",
    "        super(TCN, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(7):\n",
    "            num_inputs = input_size if i == 0 else n_hidden\n",
    "            kernel_size = 2 if i > 0 else 1\n",
    "            dilation = 2 * dilation if i > 1 else 1\n",
    "            layers += [TemporalBlock(num_inputs, n_hidden, n_hidden, kernel_size, dilation)]\n",
    "        self.conv = nn.Conv1d(n_hidden, output_size, 1)\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv.weight.data.normal_(0, 0.01)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, input_channels)\n",
    "        y1 = self.net(x.transpose(1, 2))  # Now shape: (batch_size, n_hidden, seq_len)\n",
    "        return self.conv(y1).transpose(1, 2)  # Final shape: (batch_size, seq_len, output_channels)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator: 3 to 1 Causal temporal convolutional network with skip connections.\n",
    "       This network uses 1D convolutions in order to model multiple timeseries co-dependency.\n",
    "    \"\"\" \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = TCN(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.tanh(self.net(x))\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
